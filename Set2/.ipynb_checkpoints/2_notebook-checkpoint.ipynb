{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this notebook to write your code for problem 2. You may reuse your SGD code from last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sklearn.preprocessing\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function may be useful for loading the necessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    Function loads data stored in the file filename and returns it as a numpy ndarray.\n",
    "    \n",
    "    Inputs:\n",
    "        filename: given as a string.\n",
    "        \n",
    "    Outputs:\n",
    "        Data contained in the file, returned as a numpy ndarray\n",
    "    \"\"\"\n",
    "    return np.loadtxt(filename, skiprows=1, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad(x, y, w, l, N):\n",
    "    '''returns the gradient of the regularized weight function\n",
    "    at a single point\n",
    "    inputs: x - x array of single point\n",
    "            y - classification\n",
    "            w - weight vector with bias term included\n",
    "            l - lambda for regularization\n",
    "            N - size of the training set\n",
    "    outputs: grad - gradient at the point'''\n",
    "    #norm of logistic error\n",
    "    num = -1 * y * np.exp(-1 * y * np.dot(np.transpose(w), x)) * x\n",
    "    den = (1 + np.exp(-1 * y * np.dot(np.transpose(w), x)))\n",
    "    \n",
    "    norm_grad = 2 * l / N * w    #norm of regularization term\n",
    "    grad = num/den + norm_grad    #sum to get gradient\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def log_error(X, Y, w):\n",
    "    '''calculates the logistic error across a data set; does not include regularization\n",
    "    inputs: X: A (N, D) shaped numpy array containing the data points.\n",
    "            Y: A (N, ) shaped numpy array containing the (float) labels of the data points.\n",
    "            w: A (D, ) shpaed array holding the weight vector'''\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i in range(len(Y)):\n",
    "        total_loss -= np.log(1/(1 + np.exp(-1 * Y[i] * np.dot(np.transpose(w), X[i]))))\n",
    "    \n",
    "    return total_loss/len(Y)  #return average log loss\n",
    "\n",
    "\n",
    "def SGD(X, Y, eta, N_epochs, l):\n",
    "    '''\n",
    "    Perform SGD using dataset (X, Y), initial weight vector w_start,\n",
    "    learning rate eta, and N_epochs epochs.\n",
    "    \n",
    "    Inputs:\n",
    "        X: A (N, D) shaped numpy array containing the data points.\n",
    "        Y: A (N, ) shaped numpy array containing the (float) labels of the data points.\n",
    "        eta: The step size.\n",
    "        N_epochs: The number of epochs (iterations) to run SGD.\n",
    "        \n",
    "    Outputs:\n",
    "        W: A (D, ) shaped array containing the weight vectors from all iterations.\n",
    "    '''\n",
    "    D = len(X[0])                       #number of dimensions\n",
    "    N = len(Y)                          #number of samples\n",
    "    w = (np.random.rand(D) - 0.5) / 10  #randomly initialize weights [-0.05, 0.05]\n",
    "    \n",
    "    for i in range(N_epochs):           #iterate for set number of epochs\n",
    "        perm = list(range(N))           #indices of all points\n",
    "        np.random.permutation(perm)     #shuffle order that points are added in\n",
    "        \n",
    "        for p in perm:                  #compute gradient at each point and sum\n",
    "            x = X[p]\n",
    "            y = Y[p]\n",
    "    \n",
    "            w -= eta * grad(x, y, w, l, N)   #gradient summed across all points\n",
    "               \n",
    "    \n",
    "    return w\n",
    "\n",
    "def clean(filename):\n",
    "    '''opens and prepares a dataset for use; normalizes the data\n",
    "    inputs: filename\n",
    "    outputs: X - matrix of all x coordinates normalized\n",
    "             Y - array of classifications'''\n",
    "    all_data = load_data(os.getcwd() + filename)\n",
    "    N = len(all_data)      #number of data points\n",
    "    D = len(all_data[0])   #number of dimensions of X coordinate + 1; extra for zero coor\n",
    "    Y = np.zeros(N)\n",
    "    X = np.zeros((N, D))\n",
    "    \n",
    "    for i in range(N):\n",
    "        Y[i] = all_data[i][0]          #copy y to array\n",
    "        X[i][1:] = all_data[i][1:]     #copy over the x values\n",
    "    X = sklearn.preprocessing.scale(X) #normalize X data\n",
    "    for i in range(N):\n",
    "        X[i][0] = 1                    #introduce zeroeth coordinate\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda: 1e-05\n",
      "lambda: 5e-05\n",
      "lambda: 0.00025\n",
      "lambda: 0.00125\n",
      "lambda: 0.00625\n",
      "lambda: 0.03125\n",
      "lambda: 0.15625\n",
      "lambda: 0.78125\n",
      "lambda: 3.90625\n",
      "lambda: 19.53125\n",
      "lambda: 97.65625\n",
      "lambda: 488.28125\n",
      "lambda: 2441.40625\n",
      "lambda: 12207.03125\n",
      "lambda: 61035.15625\n",
      "Ein1:  [ 0.13664302  0.13664331  0.13664476  0.136652    0.13668823  0.13686937\n",
      "  0.13777532  0.14230944  0.16485859  0.26073268  0.45748485  0.61842949\n",
      "  0.67671556  0.68962313  0.69217853]\n",
      "Ein2:  [ 0.16555676  0.16555712  0.16555896  0.16556812  0.16561394  0.16584302\n",
      "  0.16698837  0.17271108  0.20097185  0.31893153  0.53177571  0.65105499\n",
      "  0.68395786  0.69092244  0.69243043]\n",
      "Eout1:  [ 0.14363195  0.14363228  0.14363396  0.14364234  0.14368423  0.14389367\n",
      "  0.14494027  0.15015601  0.17557867  0.27667347  0.46976482  0.62374686\n",
      "  0.67932626  0.69123434  0.69256969]\n",
      "Eout2:  [ 0.3157208   0.31572105  0.31572233  0.31572871  0.31576064  0.3159203\n",
      "  0.31671883  0.32071577  0.34055713  0.4238586   0.57780252  0.66357526\n",
      "  0.68712476  0.69179778  0.69277964]\n",
      "norm1:  [ 1.38516403  1.38516162  1.38514959  1.38508941  1.38478861  1.38328645\n",
      "  1.37582176  1.33962232  1.18342007  0.75843319  0.30911753  0.0846848\n",
      "  0.01884128  0.0051637   0.00139413]\n",
      "norm2:  [ 0.94755642  0.94755471  0.94754618  0.94750353  0.9472903   0.94622546\n",
      "  0.94093292  0.91524162  0.80381593  0.49423507  0.17141876  0.04119791\n",
      "  0.00912379  0.00236744  0.00100438]\n"
     ]
    }
   ],
   "source": [
    "#create list of lambdas\n",
    "n_lambdas = 15\n",
    "lambdas = [0.00001]\n",
    "for i in range(n_lambdas - 1):\n",
    "    lambdas.append(5*lambdas[-1]) #each l_(n+1) = 5 * l_n\n",
    "lambdas = np.array(lambdas)  \n",
    "\n",
    "X_train1, Y_train1 = clean('/data/wine_training1.txt')\n",
    "X_train2, Y_train2 = clean('/data/wine_training2.txt')\n",
    "X_test, Y_test = clean('/data/wine_testing.txt')\n",
    "\n",
    "#arrays for holding performance data\n",
    "Ein1 = np.zeros(n_lambdas)    #in sample error training set 1\n",
    "Ein2 = np.zeros(n_lambdas)    #in sample error training set 2\n",
    "Eout1 = np.zeros(n_lambdas)   #out of sample error; weights trained on set 1\n",
    "Eout2 = np.zeros(n_lambdas)   #out of sample error; weights trained on set 2\n",
    "norm1 = np.zeros(n_lambdas)   #l2 norms of training set 1\n",
    "norm2 = np.zeros(n_lambdas)   #l2 norms of training set 2\n",
    "\n",
    "#training constants\n",
    "N_epochs = 20000\n",
    "eta = 5 * 10 ** -4\n",
    "\n",
    "#loop for performing SGD\n",
    "for i in range(n_lambdas):\n",
    "    print(\"lambda:\", lambdas[i])\n",
    "    w1 = SGD(X_train1, Y_train1, eta, N_epochs, lambdas[i])\n",
    "    w2 = SGD(X_train2, Y_train2, eta, N_epochs, lambdas[i])\n",
    "\n",
    "    Ein1[i] = log_error(X_train1, Y_train1, w1)\n",
    "    Ein2[i] = log_error(X_train2, Y_train2, w2)\n",
    "    \n",
    "    Eout1[i] = log_error(X_test, Y_test, w1)\n",
    "    Eout2[i] = log_error(X_test, Y_test, w2)\n",
    "    \n",
    "    norm1[i] = np.linalg.norm(w1)\n",
    "    norm2[i] = np.linalg.norm(w2)\n",
    "    \n",
    "print(\"Ein1: \", Ein1)\n",
    "print(\"Ein2: \", Ein2)\n",
    "print(\"Eout1: \", Eout1)\n",
    "print(\"Eout2: \", Eout2)\n",
    "print(\"norm1: \", norm1)\n",
    "print(\"norm2: \", norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#save sum of errors so we don't have to re-run them\n",
    "'''\n",
    "Ein1 = np.array([0.2146585, 0.21524418, 0.21668717, 0.22561476, 0.27163838, 0.53019839, 1.621084,\\\n",
    "        4.65662977, 11.81885494, 25.66433039, 45.748442, 61.84294861, 67.67155562, \\\n",
    "        68.96231347, 69.21785345])\n",
    "\n",
    "Ein2 = np.array([0.08520275, 0.0855433, 0.08640661, 0.08982212, 0.10875647, 0.21673251,\\\n",
    "        0.67943977, 2.02168509, 5.40919338, 12.46047605, 21.2709819, \\\n",
    "        26.04219946, 27.3583145, 27.63689749, 27.69721733])\n",
    "\n",
    "Eout1 = np.array([1.67735905, 1.65388723, 1.64668242, 1.65987852, 1.61788035, 1.59147899, 1.67863178, \\\n",
    "         2.28327455, 4.12369628, 8.19766758, 14.0929275, 18.71240591, 20.37978767, 20.73703021,\\\n",
    "         20.77709055])\n",
    "\n",
    "Eout2 = np.array([3.74384481, 3.73665792, 3.73108212, 3.69338096, 3.7418032, 3.71604705, \\\n",
    "         3.90808298, 5.15324003, 7.92488707, 12.41308748, 17.33397615, 19.90725766, \\\n",
    "         20.6137426, 20.75393352, 20.78338915])\n",
    "\n",
    "Ein1 = Ein1/100   #divide by number of data points to get average\n",
    "Ein2 = Ein2/40\n",
    "Eout1 = Eout1/30\n",
    "Eout2 = Eout2/30'''\n",
    "\n",
    "plt.figure(\"Ein\")\n",
    "plt.title(\"Ein\")\n",
    "plt.xlabel(\"log lambda\")\n",
    "plt.ylabel(\"Log loss Error\")\n",
    "plt.scatter(log_lambda, Ein1, color = 'g')\n",
    "plt.scatter(log_lambda, Ein2, color = 'b')\n",
    "plt.legend([\"Set1\", \"Set2\"])\n",
    "plt.grid(True)\n",
    "plt.xlim(-15, 20)\n",
    "plt.savefig(\"Ein_reg\")\n",
    "\n",
    "plt.figure(\"Eout\")\n",
    "plt.title(\"Eout\")\n",
    "plt.xlabel(\"log lambda\")\n",
    "plt.ylabel(\"Log loss Error\")\n",
    "plt.scatter(log_lambda, Eout1, color = 'g')\n",
    "plt.scatter(log_lambda, Eout2, color = 'b')\n",
    "plt.legend([\"Set1\", \"Set2\"])\n",
    "plt.grid(True)\n",
    "plt.xlim(-15, 20)\n",
    "plt.savefig(\"Eout_reg\")\n",
    "\n",
    "\n",
    "plt.figure(\"L2 norm\")\n",
    "plt.title(\"L2 norm\")\n",
    "plt.xlabel(\"log lambda\")\n",
    "plt.ylabel(\"Log loss Error\")\n",
    "plt.scatter(log_lambda, norm1, color = 'g')\n",
    "plt.scatter(log_lambda, norm2, color = 'b')\n",
    "plt.legend([\"Set1\", \"Set2\"])\n",
    "plt.grid(True)\n",
    "plt.xlim(-15, 20)\n",
    "plt.savefig(\"norms_reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "40\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "X_train1, Y_train1 = clean('/data/wine_training1.txt')\n",
    "X_train2, Y_train2 = clean('/data/wine_training2.txt')\n",
    "X_test, Y_test = clean('/data/wine_testing.txt')\n",
    "\n",
    "print(len(Y_train1))\n",
    "print(len(Y_train2))\n",
    "print(len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
